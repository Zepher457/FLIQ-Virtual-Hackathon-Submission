{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ef808767",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: qiskit in /opt/homebrew/lib/python3.10/site-packages (1.2.4)\n",
      "Requirement already satisfied: torch in /opt/homebrew/lib/python3.10/site-packages (2.3.0)\n",
      "Requirement already satisfied: numpy in /opt/homebrew/lib/python3.10/site-packages (1.26.4)\n",
      "Requirement already satisfied: pandas in /opt/homebrew/lib/python3.10/site-packages (2.2.1)\n",
      "Requirement already satisfied: rustworkx>=0.15.0 in /opt/homebrew/lib/python3.10/site-packages (from qiskit) (0.15.1)\n",
      "Requirement already satisfied: scipy>=1.5 in /opt/homebrew/lib/python3.10/site-packages (from qiskit) (1.14.1)\n",
      "Requirement already satisfied: sympy>=1.3 in /opt/homebrew/lib/python3.10/site-packages (from qiskit) (1.11.1)\n",
      "Requirement already satisfied: dill>=0.3 in /opt/homebrew/lib/python3.10/site-packages (from qiskit) (0.3.7)\n",
      "Requirement already satisfied: python-dateutil>=2.8.0 in /opt/homebrew/lib/python3.10/site-packages (from qiskit) (2.8.2)\n",
      "Requirement already satisfied: stevedore>=3.0.0 in /opt/homebrew/lib/python3.10/site-packages (from qiskit) (4.1.1)\n",
      "Requirement already satisfied: typing-extensions in /opt/homebrew/lib/python3.10/site-packages (from qiskit) (4.8.0)\n",
      "Requirement already satisfied: symengine<0.14,>=0.11 in /opt/homebrew/lib/python3.10/site-packages (from qiskit) (0.13.0)\n",
      "Requirement already satisfied: filelock in /opt/homebrew/lib/python3.10/site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: networkx in /opt/homebrew/lib/python3.10/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /Users/sunil/Library/Python/3.10/lib/python/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /opt/homebrew/lib/python3.10/site-packages (from torch) (2023.12.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/homebrew/lib/python3.10/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/homebrew/lib/python3.10/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/homebrew/lib/python3.10/site-packages (from python-dateutil>=2.8.0->qiskit) (1.16.0)\n",
      "Requirement already satisfied: pbr!=2.1.0,>=2.0.0 in /opt/homebrew/lib/python3.10/site-packages (from stevedore>=3.0.0->qiskit) (5.11.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/homebrew/lib/python3.10/site-packages (from sympy>=1.3->qiskit) (1.2.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/sunil/Library/Python/3.10/lib/python/site-packages (from jinja2->torch) (2.1.3)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.10 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install qiskit torch numpy pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf404065",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Library imports\n",
    "%matplotlib inline\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import pennylane as qml\n",
    "from IPython import display\n",
    "import progressbar\n",
    "from PIL import Image\n",
    "\n",
    "# Pytorch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set the random seed for reproducibility\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5808d442",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 │ Train loss 0.4450 │ Train acc 80.00%\n",
      "Epoch 02 │ Train loss 0.2340 │ Train acc 91.43%\n",
      "Epoch 03 │ Train loss 0.1575 │ Train acc 94.29%\n",
      "Epoch 04 │ Train loss 0.1168 │ Train acc 96.04%\n",
      "Epoch 05 │ Train loss 0.0912 │ Train acc 96.26%\n",
      "Epoch 06 │ Train loss 0.0667 │ Train acc 97.58%\n",
      "Epoch 07 │ Train loss 0.0499 │ Train acc 98.46%\n",
      "Epoch 08 │ Train loss 0.0355 │ Train acc 99.56%\n",
      "Epoch 09 │ Train loss 0.0297 │ Train acc 99.78%\n",
      "Epoch 10 │ Train loss 0.0204 │ Train acc 100.00%\n",
      "Epoch 11 │ Train loss 0.0138 │ Train acc 100.00%\n",
      "Epoch 12 │ Train loss 0.0106 │ Train acc 100.00%\n",
      "Epoch 13 │ Train loss 0.0080 │ Train acc 100.00%\n",
      "Epoch 14 │ Train loss 0.0063 │ Train acc 100.00%\n",
      "Epoch 15 │ Train loss 0.0043 │ Train acc 100.00%\n",
      "Epoch 16 │ Train loss 0.0030 │ Train acc 100.00%\n",
      "Epoch 17 │ Train loss 0.0033 │ Train acc 100.00%\n",
      "Epoch 18 │ Train loss 0.0017 │ Train acc 100.00%\n",
      "Epoch 19 │ Train loss 0.0012 │ Train acc 100.00%\n",
      "Epoch 20 │ Train loss 0.0009 │ Train acc 100.00%\n",
      "Epoch 21 │ Train loss 0.0008 │ Train acc 100.00%\n",
      "Epoch 22 │ Train loss 0.0007 │ Train acc 100.00%\n",
      "Epoch 23 │ Train loss 0.0004 │ Train acc 100.00%\n",
      "Epoch 24 │ Train loss 0.0003 │ Train acc 100.00%\n",
      "Epoch 25 │ Train loss 0.0003 │ Train acc 100.00%\n",
      "\n",
      "Test loss 0.2348 │ Test accuracy 91.23%\n"
     ]
    }
   ],
   "source": [
    "#Prep Training Data (Breast Cancer Wisconsin)\n",
    "\n",
    "#Load the data file\n",
    "df = pd.read_csv(\"data/wdbc.data\", header=None)\n",
    "\n",
    "\n",
    "#Assign column names\n",
    "columns = ['id', 'diagnosis'] + [f'feature_{i}' for i in range(1, 31)]\n",
    "df.columns = columns\n",
    "#Drop the ID column\n",
    "df = df.drop(columns=['id'])\n",
    "\n",
    "#Encode diagnosis: M = 1, B = 0\n",
    "df['diagnosis'] = df['diagnosis'].map({'M': 1, 'B': 0})\n",
    "\n",
    "#Convert to numpy arrays\n",
    "X = df.drop(columns=['diagnosis']).values.astype(np.float32)\n",
    "Y = df['diagnosis'].values.astype(np.float32).reshape(-1, 1)\n",
    "\n",
    "#Normalize features manually (z-score)\n",
    "mean = X.mean(axis=0)\n",
    "std = X.std(axis=0)\n",
    "X = (X - mean) / std\n",
    "\n",
    "#Convert to torch tensors\n",
    "X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "Y_tensor = torch.tensor(Y, dtype=torch.float32)\n",
    "\n",
    "#Manual train/test split (80/20)\n",
    "num_samples = X_tensor.shape[0]\n",
    "indices = torch.randperm(num_samples)\n",
    "\n",
    "split_idx = int(num_samples * 0.8)\n",
    "train_indices = indices[:split_idx]\n",
    "test_indices = indices[split_idx:]\n",
    "X_train = X_tensor[train_indices]\n",
    "Y_train = Y_tensor[train_indices]\n",
    "X_test = X_tensor[test_indices]\n",
    "Y_test = Y_tensor[test_indices]\n",
    "\n",
    "#Idea: build a Quantum CNN (since this is a classification task), facilitating non-linear feed-forward parts with QSVT\n",
    "#Architecture: Classical Dense (30 -> 8) -> tanh to map data to [-1,1]\n",
    "                \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# ---------------------------------------\n",
    "# 1.  Data-set and data-loader\n",
    "# ---------------------------------------\n",
    "batch_size = 1\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    TensorDataset(X_train, Y_train), batch_size=batch_size, shuffle=True\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    TensorDataset(X_test, Y_test), batch_size=batch_size, shuffle=False\n",
    ")\n",
    "\n",
    "# ---------------------------------------\n",
    "# 2.  A tiny 1-D CNN for 30 features\n",
    "# ---------------------------------------\n",
    "class BreastCancerCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels=1,  out_channels=16, kernel_size=3, padding=1)\n",
    "        self.bn1   = nn.BatchNorm1d(16)\n",
    "        self.conv2 = nn.Conv1d(16, 32, kernel_size=3, padding=1)\n",
    "        self.bn2   = nn.BatchNorm1d(32)\n",
    "        self.pool  = nn.MaxPool1d(2)\n",
    "        self.fc1   = nn.Linear(32 * 7, 64)   # 30 → pool → 15 → conv → pool → 7\n",
    "        self.fc2   = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)                  # (B, 1, 30)\n",
    "        x = self.pool(F.relu(self.bn1(self.conv1(x))))  # (B,16,15)\n",
    "        x = self.pool(F.relu(self.bn2(self.conv2(x))))  # (B,32,7)\n",
    "        x = x.flatten(1)                                    # (B,32*7)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return torch.sigmoid(self.fc2(x))                   # (B,1)\n",
    "\n",
    "diagnosis_model = BreastCancerCNN()\n",
    "\n",
    "# ---------------------------------------\n",
    "# 3.  Loss, optimiser\n",
    "# ---------------------------------------\n",
    "loss_fn   = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0002)\n",
    "\n",
    "# ---------------------------------------\n",
    "# 4.  Training loop (mini-batch SGD)\n",
    "# ---------------------------------------\n",
    "n_epochs = 25\n",
    "for epoch in range(n_epochs):\n",
    "    diagnosis_model.train()\n",
    "    running_loss, correct = 0.0, 0\n",
    "\n",
    "    for xb, yb in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        preds = diagnosis_model(xb)\n",
    "        loss  = loss_fn(preds, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * xb.size(0)\n",
    "        correct      += ((preds > 0.5).float() == yb).sum().item()\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    epoch_acc  = correct      / len(train_loader.dataset) * 100\n",
    "    print(f\"Epoch {epoch+1:02d} │ Train loss {epoch_loss:.4f} │ Train acc {epoch_acc:.2f}%\")\n",
    "\n",
    "# ---------------------------------------\n",
    "# 5.  Testing / evaluation loop\n",
    "# ---------------------------------------\n",
    "diagnosis_model.eval()\n",
    "test_loss, correct = 0.0, 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for xb, yb in test_loader:\n",
    "        preds = diagnosis_model(xb)\n",
    "        test_loss += loss_fn(preds, yb).item() * xb.size(0)\n",
    "        correct   += ((preds > 0.5).float() == yb).sum().item()\n",
    "\n",
    "test_loss /= len(test_loader.dataset)\n",
    "test_acc  = correct / len(test_loader.dataset) * 100\n",
    "print(f\"\\nTest loss {test_loss:.4f} │ Test accuracy {test_acc:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea61c28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16193"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "count_parameters(diagnosis_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c5f99cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.8636\n",
      "Recall:    0.9048\n",
      "F1-score:  0.8837\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# Ensure labels are 1D integer tensors\n",
    "Y_test = Y_test.view(-1).long()   # [N,1] → [N]\n",
    "\n",
    "diagnosis_model.eval()\n",
    "\n",
    "y_trues = []\n",
    "y_preds = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(X_test.size(0)):\n",
    "        x = X_test[i].to(device)           # single sample: [features]\n",
    "        y_true = Y_test[i].item()          # scalar 0/1\n",
    "        \n",
    "        # forward expects a batch, so add a batch dim:\n",
    "        out = diagnosis_model(x.unsqueeze(0))        # → shape [1, 1] or [1,2]\n",
    "        \n",
    "        # if you have a single-logit head:\n",
    "        if out.dim()==2 and out.shape[1]==1:\n",
    "            prob = out.item()\n",
    "            pred = 1 if prob >= 0.5 else 0\n",
    "        else:\n",
    "            # two-logit head\n",
    "            pred = out.argmax(dim=1).item()\n",
    "        \n",
    "        y_trues.append(y_true)\n",
    "        y_preds.append(pred)\n",
    "\n",
    "# now compute metrics\n",
    "precision = precision_score(y_trues, y_preds)\n",
    "recall    = recall_score(y_trues, y_preds)\n",
    "f1        = f1_score(y_trues, y_preds)\n",
    "\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall:    {recall:.4f}\")\n",
    "print(f\"F1-score:  {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d28258",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0.0: 0.9122807017543859,\n",
       " 0.01: 0.9035087719298246,\n",
       " 0.05: 0.7631578947368421,\n",
       " 0.1: 0.6140350877192983,\n",
       " 0.2: 0.41228070175438597}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def fgsm_attack(model, x, y, epsilon, device):\n",
    "    x_adv = x.clone().detach().to(device).requires_grad_(True)\n",
    "    out = model(x_adv)\n",
    "\n",
    "\n",
    "    loss = F.binary_cross_entropy_with_logits(out, y.unsqueeze(0).float().to(device))\n",
    "    model.zero_grad()\n",
    "    loss.backward()\n",
    "    \n",
    "    x_adv = x_adv + epsilon * x_adv.grad.sign()\n",
    "    return x_adv.detach()\n",
    "\n",
    "def evaluate_robustness(model, device, X_test, Y_test, epsilons=[0.0,0.01,0.05,0.1,0.2]):\n",
    "    model.eval()\n",
    "    results = {}\n",
    "    ds = TensorDataset(X_test, Y_test.view(-1))\n",
    "    loader = DataLoader(ds, batch_size=1, shuffle=False)\n",
    "    \n",
    "    for eps in epsilons:\n",
    "        preds = []\n",
    "        trues = []\n",
    "        for x, y in loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            \n",
    "            if eps == 0.0:\n",
    "                x_adv = x\n",
    "            else:\n",
    "                x_adv = fgsm_attack(model, x, y, eps, device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                out = model(x_adv)\n",
    "                if out.dim()==2 and out.size(1)==1:\n",
    "                    out = out.view(-1)\n",
    "                prob = out   # [1]\n",
    "                pred = (prob >= 0.5).long().item()\n",
    "            \n",
    "            preds.append(pred)\n",
    "            trues.append(y.item())\n",
    "        \n",
    "        results[eps] = accuracy_score(trues, preds)\n",
    "    return results\n",
    "evaluate_robustness(diagnosis_model, torch.device('cpu'), X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f7e638",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zeros (class 0): 744\n",
      "Ones  (class 1): 855\n",
      "(1599, 11) (1599, 1)\n",
      "Epoch 01 │ Train loss 0.6202 │ Train acc 65.68%\n",
      "Epoch 02 │ Train loss 0.5418 │ Train acc 73.96%\n",
      "Epoch 03 │ Train loss 0.5113 │ Train acc 75.14%\n",
      "Epoch 04 │ Train loss 0.4933 │ Train acc 76.54%\n",
      "Epoch 05 │ Train loss 0.4747 │ Train acc 76.78%\n",
      "Epoch 06 │ Train loss 0.4614 │ Train acc 78.81%\n",
      "Epoch 07 │ Train loss 0.4485 │ Train acc 79.75%\n",
      "Epoch 08 │ Train loss 0.4359 │ Train acc 80.38%\n",
      "Epoch 09 │ Train loss 0.4247 │ Train acc 80.22%\n",
      "Epoch 10 │ Train loss 0.4120 │ Train acc 82.10%\n",
      "Epoch 11 │ Train loss 0.4004 │ Train acc 81.70%\n",
      "Epoch 12 │ Train loss 0.3955 │ Train acc 82.72%\n",
      "Epoch 13 │ Train loss 0.3834 │ Train acc 83.11%\n",
      "Epoch 14 │ Train loss 0.3765 │ Train acc 83.97%\n",
      "Epoch 15 │ Train loss 0.3669 │ Train acc 84.99%\n",
      "Epoch 16 │ Train loss 0.3534 │ Train acc 85.07%\n",
      "Epoch 17 │ Train loss 0.3492 │ Train acc 85.07%\n",
      "Epoch 18 │ Train loss 0.3409 │ Train acc 85.46%\n",
      "Epoch 19 │ Train loss 0.3318 │ Train acc 86.00%\n",
      "Epoch 20 │ Train loss 0.3231 │ Train acc 87.02%\n",
      "Epoch 21 │ Train loss 0.3169 │ Train acc 86.63%\n",
      "Epoch 22 │ Train loss 0.3091 │ Train acc 86.63%\n",
      "Epoch 23 │ Train loss 0.2962 │ Train acc 87.88%\n",
      "Epoch 24 │ Train loss 0.2942 │ Train acc 88.90%\n",
      "Epoch 25 │ Train loss 0.2868 │ Train acc 87.96%\n",
      "Epoch 26 │ Train loss 0.2773 │ Train acc 89.52%\n",
      "Epoch 27 │ Train loss 0.2695 │ Train acc 89.29%\n",
      "Epoch 28 │ Train loss 0.2635 │ Train acc 90.15%\n",
      "Epoch 29 │ Train loss 0.2598 │ Train acc 90.07%\n",
      "Epoch 30 │ Train loss 0.2475 │ Train acc 90.23%\n",
      "\n",
      "Test loss 0.8126 │ Test accuracy 66.25%\n"
     ]
    }
   ],
   "source": [
    "#Prep Training Data (Breast Cancer Wisconsin)\n",
    "\n",
    "#Prep Training Data (Breast Cancer Wisconsin)\n",
    "\n",
    "#Load the data file\n",
    "df = pd.read_csv(\"data/winequality-red.csv\", sep = ';', header=0)\n",
    "\n",
    "Y = (df['quality'] >= 6).astype(int).values.reshape(-1,1)\n",
    "X = df.drop(['quality'], axis=1).values\n",
    "\n",
    "\n",
    "mean = X.mean(axis=0)\n",
    "std = X.std(axis=0)\n",
    "X = (X - mean) / std\n",
    "\n",
    "#Convert to torch tensors\n",
    "X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "Y_tensor = torch.tensor(Y, dtype=torch.float32)\n",
    "y = Y_tensor.view(-1).long()     # flatten to shape [N] and cast to int\n",
    "counts = torch.bincount(y)       # bincount over 0,1\n",
    "print(f\"Zeros (class 0): {counts[0].item()}\")\n",
    "print(f\"Ones  (class 1): {counts[1].item()}\")\n",
    "\n",
    "#Manual train/test split (80/20)\n",
    "num_samples = X_tensor.shape[0]\n",
    "indices = torch.randperm(num_samples)\n",
    "\n",
    "split_idx = int(num_samples * 0.8)\n",
    "train_indices = indices[:split_idx]\n",
    "test_indices = indices[split_idx:]\n",
    "X_train = X_tensor[train_indices]\n",
    "Y_train = Y_tensor[train_indices]\n",
    "X_test = X_tensor[test_indices]\n",
    "Y_test = Y_tensor[test_indices]\n",
    "print(X.shape, Y.shape)\n",
    "\n",
    "#Idea: build a Quantum CNN (since this is a classification task), facilitating non-linear feed-forward parts with QSVT\n",
    "#Architecture: Classical Dense (30 -> 8) -> tanh to map data to [-1,1]\n",
    "                \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# ---------------------------------------\n",
    "# 1.  Data-set and data-loader\n",
    "# ---------------------------------------\n",
    "batch_size = 1\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    TensorDataset(X_train, Y_train), batch_size=batch_size, shuffle=True\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    TensorDataset(X_test, Y_test), batch_size=batch_size, shuffle=False\n",
    ")\n",
    "\n",
    "# ---------------------------------------\n",
    "# 2.  A tiny 1-D CNN for 30 features\n",
    "# ---------------------------------------\n",
    "class WineCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels=1,  out_channels=16, kernel_size=3, padding=1)\n",
    "        self.bn1   = nn.BatchNorm1d(16)\n",
    "        self.conv2 = nn.Conv1d(16, 32, kernel_size=3, padding=1)\n",
    "        self.bn2   = nn.BatchNorm1d(32)\n",
    "        self.pool  = nn.MaxPool1d(2)\n",
    "        self.fc1   = nn.Linear(32 * 2, 32)   # 30 → pool → 15 → conv → pool → 7\n",
    "        self.fc2   = nn.Linear(32, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)                  # (B, 1, 30)\n",
    "        x = self.pool(F.relu(self.bn1(self.conv1(x))))  # (B,16,15)\n",
    "        x = self.pool(F.relu(self.bn2(self.conv2(x))))  # (B,32,7)\n",
    "        x = x.flatten(1)                                    # (B,32*7)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return torch.sigmoid(self.fc2(x))                   # (B,1)\n",
    "\n",
    "wine_model = WineCNN()\n",
    "\n",
    "# ---------------------------------------\n",
    "# 3.  Loss, optimiser\n",
    "# ---------------------------------------\n",
    "loss_fn   = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0002)\n",
    "\n",
    "# ---------------------------------------\n",
    "# 4.  Training loop (mini-batch SGD)\n",
    "# ---------------------------------------\n",
    "n_epochs = 30\n",
    "for epoch in range(n_epochs):\n",
    "    wine_model.train()\n",
    "    running_loss, correct = 0.0, 0\n",
    "\n",
    "    for xb, yb in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        preds = wine_model(xb)\n",
    "        loss  = loss_fn(preds, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * xb.size(0)\n",
    "        correct      += ((preds > 0.5).float() == yb).sum().item()\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    epoch_acc  = correct      / len(train_loader.dataset) * 100\n",
    "    print(f\"Epoch {epoch+1:02d} │ Train loss {epoch_loss:.4f} │ Train acc {epoch_acc:.2f}%\")\n",
    "\n",
    "# ---------------------------------------\n",
    "# 5.  Testing / evaluation loop\n",
    "# ---------------------------------------\n",
    "wine_model.eval()\n",
    "test_loss, correct = 0.0, 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for xb, yb in test_loader:\n",
    "        preds = wine_model(xb)\n",
    "        test_loss += loss_fn(preds, yb).item() * xb.size(0)\n",
    "        correct   += ((preds > 0.5).float() == yb).sum().item()\n",
    "\n",
    "test_loss /= len(test_loader.dataset)\n",
    "test_acc  = correct / len(test_loader.dataset) * 100\n",
    "print(f\"\\nTest loss {test_loss:.4f} │ Test accuracy {test_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8cbb4bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.6761\n",
      "Recall:    0.7000\n",
      "F1-score:  0.6879\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# Ensure labels are 1D integer tensors\n",
    "Y_test = Y_test.view(-1).long()   # [N,1] → [N]\n",
    "\n",
    "wine_model.eval()\n",
    "\n",
    "y_trues = []\n",
    "y_preds = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(X_test.size(0)):\n",
    "        x = X_test[i].to(device)           # single sample: [features]\n",
    "        y_true = Y_test[i].item()          # scalar 0/1\n",
    "        \n",
    "        # forward expects a batch, so add a batch dim:\n",
    "        out = wine_model(x.unsqueeze(0))        # → shape [1, 1] or [1,2]\n",
    "        \n",
    "        # if you have a single-logit head:\n",
    "        if out.dim()==2 and out.shape[1]==1:\n",
    "            prob = out.item()\n",
    "            pred = 1 if prob >= 0.5 else 0\n",
    "        else:\n",
    "            # two-logit head\n",
    "            pred = out.argmax(dim=1).item()\n",
    "        \n",
    "        y_trues.append(y_true)\n",
    "        y_preds.append(pred)\n",
    "\n",
    "# now compute metrics\n",
    "precision = precision_score(y_trues, y_preds)\n",
    "recall    = recall_score(y_trues, y_preds)\n",
    "f1        = f1_score(y_trues, y_preds)\n",
    "\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall:    {recall:.4f}\")\n",
    "print(f\"F1-score:  {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a785ca1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0.0: 0.6625, 0.01: 0.640625, 0.05: 0.54375, 0.1: 0.446875, 0.2: 0.296875}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_robustness(wine_model, torch.device('cpu'), X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5ce9f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3841"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_parameters(wine_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aad66be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
